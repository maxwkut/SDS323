---
title: "SDS323: Exercise 1"
author: "Max Kutschinski (mwk556)"
date: "2/10/2020"
output: md_document

 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
***
## Data Visualization: Flights at ABIA
***



<font size="10"> Flight Cancellation Patterns </font>



```{r, include=F}
library(ggplot2)
library(knitr)
library(mosaic)
library(dplyr)
library(gridExtra)
ABIA = read.csv("ABIA.csv")

```
```{r, echo=F}

#Pct of cancelled flights per week
PctDayOfWeek=ABIA%>%
  group_by(DayOfWeek)%>%
  summarize(canc.mean=100*mean(Cancelled))

plotC1= ggplot(PctDayOfWeek,aes(DayOfWeek, canc.mean, fill= DayOfWeek==2))+
  geom_bar(stat="identity")+
  scale_fill_manual(values=c("grey24","indianred4"))+
  labs(title= "Percentage of cancelled flights over the week", x= "Day", y="Flights cancelled (%)")+
  scale_x_discrete(limits=c("Mo","Tu","We","Th","Fr","Sa","So"))+
  guides(fill=F)




#Pct of cancelled flights per month
PctDayOfMonth=ABIA%>%
  group_by(DayofMonth)%>%
  summarize(canc.mean=100*mean(Cancelled))

plotC2= ggplot(PctDayOfMonth,aes(DayofMonth, canc.mean, fill= canc.mean>2))+
  geom_bar(stat="identity")+
  coord_flip()+
  scale_fill_manual(values=c("grey24","indianred4"))+
  labs(title= "Percentage of cancelled flights over the month", x= "Day", y="Flights cancelled (%)")+
  scale_x_discrete(limits=c(1:31))+
  guides(fill=F)
  

#number of cancelled flights per year
#Pct of cancelled flights per month
PctMonth=ABIA%>%
  group_by(Month)%>%
  summarize(canc.mean=100*mean(Cancelled))

plotC3 = ggplot(PctMonth,aes(Month, canc.mean, fill=canc.mean>2.4))+
  geom_bar(stat="identity")+
  scale_fill_manual(values=c("grey24","indianred4"))+
  labs(title= "Percentage of cancelled flights over the year", x= "Month", y="Flights cancelled (%)")+
  guides(fill=F)+
  scale_x_discrete(limits=c("Jan","Feb","Mar","Apr","May","Jun","Jul","Aug","Sep","Oct","Nov","Dec"))
plotC1
plotC2
plotC3
```

<font size="10"> Flight Delay Patterns </font>

```{r, echo=F}


###ArrDelay
ArrDelay_mean= ABIA%>%
  group_by(UniqueCarrier)%>%
  summarize(ArrDelay.mean=mean(ArrDelay, na.rm= T))

ArrDelay_mean=mutate(ArrDelay_mean, ArrDelay.z = (ArrDelay.mean-mean(ArrDelay.mean))/sd(ArrDelay.mean))

plotA1= ggplot(ArrDelay_mean, aes(x=reorder(UniqueCarrier, ArrDelay.z), y=ArrDelay.z))+
  geom_bar(stat='identity', fill="grey24")+
  labs(title="Arrival delay by carrier",
       y="Arrival delay (z-score)",
       x="Carrier")+
  coord_flip()


ArrDelay1_mean= ABIA%>%
  group_by(DayOfWeek)%>%
  summarize(ArrDelay1.mean=mean(ArrDelay, na.rm= T))

plotA2= ggplot(ArrDelay1_mean, aes(DayOfWeek,ArrDelay1.mean, fill=ArrDelay1.mean>8))+
  geom_bar(stat='identity')+
  scale_fill_manual(values=c("grey24","indianred4"))+
  labs(title= "Arrival delay over the week", x= "Day", y="Mean arrival delay (minutes)")+
  guides(fill=F)+
  scale_x_discrete(limits=c("Mo","Tu","We","Th","Fr","Sa","So"))


grid.arrange(plotA1, plotA2, ncol=2)

###DepDelay
DepDelay_mean= ABIA%>%
  group_by(UniqueCarrier)%>%
  summarize(DepDelay.mean=mean(DepDelay, na.rm= T))

DepDelay_mean=mutate(DepDelay_mean, DepDelay.z = (DepDelay.mean-mean(DepDelay.mean))/sd(DepDelay.mean))

plotD1= ggplot(DepDelay_mean, aes(x=reorder(UniqueCarrier, DepDelay.z), y=DepDelay.z))+
  geom_bar(stat='identity', fill="grey24")+
  labs(title="Departure delay by carrier",
       y="Departure delay (z-score)",
       x="Carrier")+
  coord_flip()


DepDelay1_mean= ABIA%>%
  group_by(DayOfWeek)%>%
  summarize(DepDelay1.mean=mean(DepDelay, na.rm= T))

plotD2= ggplot(DepDelay1_mean, aes(DayOfWeek,DepDelay1.mean, fill=DepDelay1.mean>11))+
  geom_bar(stat='identity')+
  scale_fill_manual(values=c("grey24","indianred4"))+
  labs(title= "Departure delay over the week", x= "Day", y="Mean departure delay (minutes)")+
  guides(fill=F)+
  scale_x_discrete(limits=c("Mo","Tu","We","Th","Fr","Sa","So"))

grid.arrange(plotD1, plotD2, ncol=2)

##########

test= ABIA%>%
  filter(Dest!="AUS")%>%
  filter(Dest!="DSM")%>%
  filter(Dest!="DTW")%>%
  filter(Dest!="ORF")%>%
  group_by(Dest)%>%
  summarize(ArrDelayavg=mean(ArrDelay,na.rm=T))%>%
  arrange(desc(ArrDelayavg))%>%
  filter(ArrDelayavg>9.0)

ggplot(test,aes(x=reorder(Dest,ArrDelayavg),ArrDelayavg))+
  geom_bar(stat='identity', fill="grey24")+
  labs(title= "Top ten airport destinations to avoid when flying from Austin",caption="*excluding low frequency destinations (DSM,DTW,ORF).",subtitle = "Airports with the longest average arrival delays", x= "Destination", y="Average arrival delay (minutes)")



```

***

## Regression Practice

***
_Used creatinine.csv, together with knowledge of linear regression, to answer the following three questions:_

_1. What creatinine clearance rate should we expect, on average, for a 55-year-old?_

_2. How does creatinine clearance rate change with age? (This should be a number with units ml/minute per year.)_

_3. Whose creatinine clearance rate is healthier (higher) for their age: a 40-year-old with a rate of 135, or a 60-year-old with a rate of 112?_



***
```{r, include = F}
library(ggplot2)
creatinine = read.csv("creatinine.csv", header= T)
```

```{r}
ggplot(data=creatinine, aes(age, creatclear)) +
  geom_point() +
  geom_smooth(method="lm", color="indianred4",se= F)+
  labs(title = "Creatinine Clearance Rate vs Age")

```

**1. What creatinine clearance rate should we expect, on average, for a 55-year-old?**


```{r}
lm1 = lm(creatclear ~ age, data= creatinine)
new_data= data.frame(age = 55)
predict(lm1, new_data)
```
Therefore, a 55 year old is predicted to have a creatinine clearance rate of 113.723 ml/minute.

**2. How does creatinine clearance rate change with age?**

The rate of change is simply the coefficient of the age variable.
```{r}
coef(lm1)
```

Thus, as age increases by one year, clearance rate is predicted to decrease by 0.62 ml/minute.
(0.62 ml/minute per year)


**3. Whose creatinine clearance rate is healthier (higher) for their age: a 40-year-old with a rate of 135, or a 60-year-old with a rate of 112?**

```{r}
pred40=predict(lm1, data.frame(age= 40))
pred60=predict(lm1, data.frame(age= 60))
obs40=135
obs60=112
error40= obs40-pred40
error60= obs60-pred60
error40
error60
```
The 40 year old has a healthier creatine clearance rate for her age, since his observed value is further above the predicted value.




***
## Green Buildings
***

```{r, include=F}
greenbuildings= read.csv("greenbuildings.csv")
```
In order to draw a meaningful conclusion from the dataset, I first cleaned-up it up a little. Clearly, the "data guru" did not control for any confounding variables such as employment growth, utilities, age, etc. 
In my ananalysis, I decided to restrict the leasing rate to a rate above 30 percent, because I found 10 percent to be a little conservative. Furthermore, I controlled for stories and class, by comparing only good quality buildings with 10 to 20 stories. Lastly, I also restricted the dataset to buildings with a size between 150,000sqft and 350,000sqft in economic areas similar to Austin (employment growth between 2 and 10 percent).
Next, I deleted potential outliers that could skew the data.


```{r, echo=F}

clean_data= greenbuildings%>%
  filter(leasing_rate>30)%>%
  filter(stories<20 &stories >10)%>%
  filter(class_a==1 | class_b==1)%>%
  filter(size>150000 & size<350000)%>%
  filter(empl_gr>2 & empl_gr<10)


outliers<-boxplot(clean_data$Rent, plot=F)$out
clean_data<-clean_data[-which(clean_data$Rent %in% outliers),] 


ggplot(clean_data, aes(factor(green_rating), Rent))+
  geom_boxplot(colour="grey24")+
  geom_jitter(alpha=0.1, colour= "indianred4")+
  labs(title="The economic value of green buildings",
       subtitle= "Comparison of rent charged by green buildings and non-green buildings",
       x= "Green rating")+
  scale_x_discrete(labels = c("No","Yes"))
  


ggplot(clean_data, aes(leasing_rate))+
  geom_density(aes(fill=factor(green_rating)), alpha =0.8)+
  labs(title= "Occupancy rates among green and non-green buildings",
       x= "Leasing rate", y="Density",
       fill= "Green rating")+
  scale_fill_discrete(labels = c("No","Yes"))


```

These two pictures confirm the economic value behind green buildings suggested by the "data guru". While I do not agree with the means he used to obtain his conclusions, I agree for two reasons with the fact that the outlined investment plan is generally a worthwhile investment. First, the rent charged for green buildings is higher on average. Second, the occupancy rates are higher than for similar non-green buildings.




***
## Milk Prices
***
First, I came up with the profit equation, which depends on the price(P), the quantity(Q), and the cost(C). Note that quantity also depends on price and can therefore be represented as Q(P).
$$Profits= (P-C)*Q(P)$$ 

Since Q(P) represents the demand curve, I used a log model to make use of the power law. This seems like a good fit, because the log transformation resembles a linear trend.
$$Q(P)=\alpha P^{\beta}$$
```{r, echo=F}
milk= read.csv("milk.csv", header = T)
plot(log(sales)~log(price), milk)
```


```{r}
lm1= lm(log(sales)~log(price), milk)
#coefficients of log model
coef(lm1)
alpha= exp(coef(lm1)[1])
beta= coef(lm1)[2]
```

The coefficients of this model turned out to be $\alpha \approx 112.24$ and $\beta \approx -1.62$
Thus,
$$Profits = (P-C)*112.24*P^{-1.62}$$
Consider Case where C=1. Then the profit maximizing price can be found by setting the first derivate equal to 0 and solving for P:
$$\frac{\partial Profits}{\partial P}=\frac{\partial}{\partial P}[(P-1)*112.24*P^{-1.62}]=0$$
$$P \approx \$2.61$$
```{r}
curve((x-1)*alpha*x^(beta), from=2, to=4)
```

Given that the cost is equal to $1, the profit maximizing price is $2.61. At this price, the net profit is equal to: $Profit=(2.61-1)*112.24*2.61^{-1.62} \approx \$38.20$.

In general, the price P* that maximizes the profits for a given per-unit cost C is:

$$\frac{\partial Profits}{\partial P}=\frac{\partial}{\partial P}[(P-C)*112.24*P^{-1.62}]=0$$
$$P^{*} \approx 2.613C$$